{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Vezbe 3 - Lekser",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGEPN7BYQfMc",
        "colab_type": "text"
      },
      "source": [
        "# Vežbe 3: Lekser\n",
        "\n",
        "[Lekser](https://en.wikipedia.org/wiki/Lexical_analysis) jer deo kompajlera koji na osnovu izvornog koda formira niz tokena. Token je uređeni par (klasa, leksema). Leksema je karakter ili ključna reč koja ima funkciju u sintaksi programskog jezika. Upravo ta funkcija određuje klasu lekseme. Lekser će pročitati izvorni kod i to je jedini put kada će se to uraditi u čitavom procesu kompajliranja. Naredne faze kompajliranja zahtevaju samo formirani niz tokena.\n",
        "\n",
        "![pp-01](https://i.postimg.cc/SNmFQ6X0/pp-01.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6idiapve0Dwr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from enum import Enum, auto"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJREGnT22DvJ",
        "colab_type": "text"
      },
      "source": [
        "Klasa **Class** definiše sve moguće klase leksema koje se mogu naći u izvornom kodu."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Ti8oqs6zHXG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Class(Enum):\n",
        "    PLUS = auto()\n",
        "    MINUS = auto()\n",
        "    STAR = auto()\n",
        "    FWDSLASH = auto()\n",
        "    PERCENT = auto()\n",
        "\n",
        "    OR = auto()\n",
        "    AND = auto()\n",
        "    NOT = auto()\n",
        "\n",
        "    EQ = auto()\n",
        "    NEQ = auto()\n",
        "    LT = auto()\n",
        "    GT = auto()\n",
        "    LTE = auto()\n",
        "    GTE = auto()\n",
        "\n",
        "    LPAREN = auto()\n",
        "    RPAREN = auto()\n",
        "    LBRACKET = auto()\n",
        "    RBRACKET = auto()\n",
        "    LBRACE = auto()\n",
        "    RBRACE = auto()\n",
        "\n",
        "    ASSIGN = auto()\n",
        "    SEMICOLON = auto()\n",
        "    COMMA = auto()\n",
        "\n",
        "    TYPE = auto()\n",
        "    INT = auto()\n",
        "    CHAR = auto()\n",
        "    STRING = auto()\n",
        "\n",
        "    IF = auto()\n",
        "    ELSE = auto()\n",
        "    WHILE = auto()\n",
        "    FOR = auto()\n",
        "\n",
        "    BREAK = auto()\n",
        "    CONTINUE = auto()\n",
        "    RETURN = auto()\n",
        "    \n",
        "    ADDRESS = auto()\n",
        "\n",
        "    ID = auto()\n",
        "    EOF = auto()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKia3bBV2k91",
        "colab_type": "text"
      },
      "source": [
        "Klasa **Token** predstavlja uređeni par (klasa, leksema).\n",
        "\n",
        "Medota **str** vraća string reprezentaciju tokena koja se koristi u procesu pronalaženja grešaka."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ok5GyT3VzI4x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Token:\n",
        "    def __init__(self, class_, lexeme):\n",
        "        self.class_ = class_\n",
        "        self.lexeme = lexeme\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"<{} {}>\".format(self.class_, self.lexeme)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66Ke6Rhy2m7-",
        "colab_type": "text"
      },
      "source": [
        "Klasa **Lekser** sadrži metode za leksičku analizu izvornog koda.\n",
        "\n",
        "Metoda **lex** formira niz tokena pozivajući metodu **next_token**.\n",
        "\n",
        "Metoda **next_token** konstruiše token odgovarajuće klase pozivajući metodu **next_char**.\n",
        "\n",
        "Metoda **next_char** pomera pokazivač na sledeći karakter.\n",
        "\n",
        "Metoda **read_keyword** konstruiše token ključne reči pod uslovom da je trenutni karakter slovo.\n",
        "\n",
        "Metoda **read_string** konstruiše token string literala pod uslovom da je trenutni karakter znak navodnika.\n",
        "\n",
        "Metoda **read_char** konstruiše token literala karaktera pod uslovom da je trenutni karakter apostrof.\n",
        "\n",
        "Metoda **read_int** konstruiše token literala celog broja pod uslovom da je trenutni karakter cifra.\n",
        "\n",
        "Metoda **read_space** ne konstruiše token, ali pomera pokazivač na prvi sledeći karakter koji nije razmak.\n",
        "\n",
        "Metoda **die** se koristi u slučaju da je lekser pročitao neočekivani karakter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2b3Imf7y_cO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Lexer:\n",
        "    def __init__(self, text):\n",
        "        self.text = text\n",
        "        self.len = len(text)\n",
        "        self.pos = -1\n",
        "\n",
        "    def read_space(self):\n",
        "        while self.pos + 1 < self.len and self.text[self.pos + 1].isspace():\n",
        "            self.next_char()\n",
        "\n",
        "    def read_int(self):\n",
        "        lexeme = self.text[self.pos]\n",
        "        while self.pos + 1 < self.len and self.text[self.pos + 1].isdigit():\n",
        "            lexeme += self.next_char()\n",
        "        return int(lexeme)\n",
        "\n",
        "    def read_char(self):\n",
        "        self.pos += 1\n",
        "        lexeme = self.text[self.pos]\n",
        "        self.pos += 1\n",
        "        return lexeme\n",
        "\n",
        "    def read_string(self):\n",
        "        lexeme = ''\n",
        "        while self.pos + 1 < self.len and self.text[self.pos + 1] != '\"':\n",
        "            lexeme += self.next_char()\n",
        "        self.pos += 1\n",
        "        return lexeme\n",
        "\n",
        "    def read_keyword(self):\n",
        "        lexeme = self.text[self.pos]\n",
        "        while self.pos + 1 < self.len and self.text[self.pos + 1].isalnum():\n",
        "            lexeme += self.next_char()\n",
        "        if lexeme == 'if':\n",
        "            return Token(Class.IF, lexeme)\n",
        "        elif lexeme == 'else':\n",
        "            return Token(Class.ELSE, lexeme)\n",
        "        elif lexeme == 'while':\n",
        "            return Token(Class.WHILE, lexeme)\n",
        "        elif lexeme == 'for':\n",
        "            return Token(Class.FOR, lexeme)\n",
        "        elif lexeme == 'break':\n",
        "            return Token(Class.BREAK, lexeme)\n",
        "        elif lexeme == 'continue':\n",
        "            return Token(Class.CONTINUE, lexeme)\n",
        "        elif lexeme == 'return':\n",
        "            return Token(Class.RETURN, lexeme)\n",
        "        elif lexeme == 'int' or lexeme == 'char' or lexeme == 'void':\n",
        "            return Token(Class.TYPE, lexeme)\n",
        "        return Token(Class.ID, lexeme)\n",
        "\n",
        "    def next_char(self):\n",
        "        self.pos += 1\n",
        "        if self.pos >= self.len:\n",
        "            return None\n",
        "        return self.text[self.pos]\n",
        "\n",
        "    def next_token(self):\n",
        "        self.read_space()\n",
        "        curr = self.next_char()\n",
        "        if curr is None:\n",
        "            return Token(Class.EOF, curr)\n",
        "        token = None\n",
        "        if curr.isalpha():\n",
        "            token = self.read_keyword()\n",
        "        elif curr.isdigit():\n",
        "            token = Token(Class.INT, self.read_int())\n",
        "        elif curr == '\\'':\n",
        "            token = Token(Class.CHAR, self.read_char())\n",
        "        elif curr == '\"':\n",
        "            token = Token(Class.STRING, self.read_string())\n",
        "        elif curr == '+':\n",
        "            token = Token(Class.PLUS, curr)\n",
        "        elif curr == '-':\n",
        "            token = Token(Class.MINUS, curr)\n",
        "        elif curr == '*':\n",
        "            token = Token(Class.STAR, curr)\n",
        "        elif curr == '/':\n",
        "            token = Token(Class.FWDSLASH, curr)\n",
        "        elif curr == '%':\n",
        "            token = Token(Class.PERCENT, curr)\n",
        "        elif curr == '&':\n",
        "            curr = self.next_char()\n",
        "            if curr == '&':\n",
        "                token = Token(Class.AND, '&&')\n",
        "            else:\n",
        "                token = Token(Class.ADDRESS, '&')\n",
        "                self.pos -= 1\n",
        "        elif curr == '|':\n",
        "            curr = self.next_char()\n",
        "            if curr == '|':\n",
        "                token = Token(Class.OR, '||')\n",
        "            else:\n",
        "                self.die(curr)\n",
        "        elif curr == '!':\n",
        "            curr = self.next_char()\n",
        "            if curr == '=':\n",
        "                token = Token(Class.NEQ, '!=')\n",
        "            else:\n",
        "                token = Token(Class.NOT, '!')\n",
        "                self.pos -= 1\n",
        "        elif curr == '=':\n",
        "            curr = self.next_char()\n",
        "            if curr == '=':\n",
        "                token = Token(Class.EQ, '==')\n",
        "            else:\n",
        "                token = Token(Class.ASSIGN, '=')\n",
        "                self.pos -= 1\n",
        "        elif curr == '<':\n",
        "            curr = self.next_char()\n",
        "            if curr == '=':\n",
        "                token = Token(Class.LTE, '<=')\n",
        "            else:\n",
        "                token = Token(Class.LT, '<')\n",
        "                self.pos -= 1\n",
        "        elif curr == '>':\n",
        "            curr = self.next_char()\n",
        "            if curr == '=':\n",
        "                token = Token(Class.GTE, '>=')\n",
        "            else:\n",
        "                token = Token(Class.GT, '>')\n",
        "                self.pos -= 1\n",
        "        elif curr == '(':\n",
        "            token = Token(Class.LPAREN, curr)\n",
        "        elif curr == ')':\n",
        "            token = Token(Class.RPAREN, curr)\n",
        "        elif curr == '[':\n",
        "            token = Token(Class.LBRACKET, curr)\n",
        "        elif curr == ']':\n",
        "            token = Token(Class.RBRACKET, curr)\n",
        "        elif curr == '{':\n",
        "            token = Token(Class.LBRACE, curr)\n",
        "        elif curr == '}':\n",
        "            token = Token(Class.RBRACE, curr)\n",
        "        elif curr == ';':\n",
        "            token = Token(Class.SEMICOLON, curr)\n",
        "        elif curr == ',':\n",
        "            token = Token(Class.COMMA, curr)\n",
        "        else:\n",
        "            self.die(curr)\n",
        "        return token\n",
        "\n",
        "    def lex(self):\n",
        "        tokens = []\n",
        "        while True:\n",
        "            curr = self.next_token()\n",
        "            tokens.append(curr)\n",
        "            if curr.class_ == Class.EOF:\n",
        "                break\n",
        "        return tokens\n",
        "\n",
        "    def die(self, char):\n",
        "        raise SystemExit(\"Unexpected character: {}\".format(char))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bql0mM1f63hm",
        "colab_type": "text"
      },
      "source": [
        "Testiranje implementacije"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OD_956yCyDLd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_id = 1\n",
        "path = f'/content/drive/My Drive/PP/data/test/test{test_id}.c'\n",
        "\n",
        "with open(path, 'r') as source:\n",
        "    text = source.read()\n",
        "\n",
        "    lexer = Lexer(text)\n",
        "    tokens = lexer.lex()\n",
        "\n",
        "    for t in tokens:\n",
        "        print(t)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}